{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0LM5A94IPGe",
        "outputId": "47de55df-d680-495e-ca4d-62f7eb41204b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pypdf colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-5u3wJ-8IkLY"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Initialize colorama for cross-platform colored output\n",
        "init()\n",
        "\n",
        "\n",
        "class BaseRAG:\n",
        "    \"\"\"Base RAG system with common functionality\"\"\"\n",
        "\n",
        "    def __init__(self, pdf_path: str):\n",
        "        print(f\"{Fore.BLUE}üîß Initializing RAG system...{Style.RESET_ALL}\")\n",
        "\n",
        "        # Load embedding model\n",
        "        print(\"üìö Loading embedding model...\")\n",
        "        self.embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\") # sentence transoformer, embedding: pasar a vector (frases enteras a vectores)\n",
        "\n",
        "        # Process PDF\n",
        "        print(f\"üìÑ Reading PDF: {pdf_path}\")\n",
        "        self.chunks = self._process_pdf(pdf_path) # coge fragmentos (chunks) y lo transforma en vectores\n",
        "        print(f\"‚úÇÔ∏è  Split into {len(self.chunks)} chunks\")\n",
        "\n",
        "        # Create embeddings\n",
        "        print(\"üßÆ Creating embeddings for chunks...\")\n",
        "        self.chunk_embeddings = self.embed_model.encode(self.chunks) # embeddings de fragmentos del pdf\n",
        "\n",
        "    def _process_pdf(self, pdf_path: str, chunk_size: int = 250) -> list:\n",
        "        \"\"\"Extract and chunk PDF text\"\"\"\n",
        "        # muy simple y con fragmentos de 250 sin fijerse en parseados y esas cosas\n",
        "        # cuanto mas corto se puede perder contexto. tiene que haber un punto medio\n",
        "        # se le puede configurar para que lidie con cosas como fotos\n",
        "        # aqui lo esta guardando en RAM, pero suele ser inviable. Con mas datos no cabria en memoria principal.\n",
        "        # habria que explorar Vector-Databases: FAISS, Chroma, etc...\n",
        "        reader = PdfReader(pdf_path)\n",
        "        chunks = []\n",
        "        for page in reader.pages:\n",
        "            text_page = page.extract_text()\n",
        "            chunks.extend(\n",
        "                [\n",
        "                    text_page[i : i + chunk_size]\n",
        "                    for i in range(0, len(text_page), chunk_size)\n",
        "                ]\n",
        "            )\n",
        "        return chunks\n",
        "\n",
        "    def _get_relevant_chunks(self, question: str, top_k: int = 3):\n",
        "        \"\"\"Get top-k most relevant chunks for the question\"\"\"\n",
        "        q_embedding = self.embed_model.encode(question) # coge el encoder de antes y transforma la pregunta en numeritos\n",
        "        similarities = np.dot(self.chunk_embeddings, q_embedding) # compara el vector pregunta con los fragmentos, con dot escalar\n",
        "\n",
        "        # en BERT en este caso es muy exquisito, si no esta seguro pasa de ti\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "        top_chunks = [self.chunks[i] for i in top_indices]\n",
        "        top_scores = similarities[top_indices] # los scores son comparacion de vectores\n",
        "\n",
        "        # Show similarity scores\n",
        "        for i, score in enumerate(top_scores):\n",
        "            print(\n",
        "                f\"\\n{Fore.YELLOW}Chunk {i+1} similarity: {score:.2f}{Style.RESET_ALL}\"\n",
        "            )\n",
        "            preview = textwrap.fill(top_chunks[i][:150] + \"...\", width=80)\n",
        "            print(f\"Preview: {preview}\")\n",
        "\n",
        "        return top_chunks, top_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dscG2DDrIPGh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BertRAG(BaseRAG):\n",
        "    \"\"\"BERT-based extractive QA implementation\"\"\"\n",
        "\n",
        "    def __init__(self, pdf_path: str):\n",
        "        super().__init__(pdf_path)\n",
        "\n",
        "        # Load QA model\n",
        "        print(\"üß† Loading BERT QA model...\")\n",
        "        model_name = \"deepset/roberta-base-squad2\" # coge otro modelo que se encarga de generar las respuestas\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(model_name) # el sentence embedder tiene un tokenizer anted de la frase completa devolvia un vector. Aqui hay separacion entre el tokenixer y el modelo\n",
        "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name) # hay modelos especializados, como este para question answering\n",
        "        print(f\"‚úÖ Setup complete! Ready for questions\\n\")\n",
        "\n",
        "    def answer_question(self, question: str):\n",
        "        print(f\"\\n{Fore.GREEN}‚ùì Question: {question}{Style.RESET_ALL}\")\n",
        "\n",
        "        # 1. Get relevant chunks\n",
        "        print(\"\\n1Ô∏è‚É£  Finding relevant context...\")\n",
        "        relevant_chunks, relevance_scores = self._get_relevant_chunks(question) # buscamos chunks relevantes\n",
        "\n",
        "        # 2. Combine chunks\n",
        "        print(\"\\n2Ô∏è‚É£  Combining chunks for context...\")\n",
        "        context = \" [SEP] \".join(relevant_chunks) # combino los chunks en una unica frase, lo llamamos contexto\n",
        "\n",
        "        # 3. Get answer\n",
        "        print(\"\\n3Ô∏è‚É£  Extracting answer...\")\n",
        "        inputs = self.qa_tokenizer(\n",
        "            question, context, return_tensors=\"pt\", max_length=512, truncation=True\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qa_model(**inputs) # llamo al modelo con estos inputs\n",
        "            start_scores = outputs.start_logits[0]\n",
        "            end_scores = outputs.end_logits[0] # intenta localizar donde esta la respuesta a la pregunta\n",
        "\n",
        "            start_idx = torch.argmax(start_scores)\n",
        "            end_idx = torch.argmax(end_scores)\n",
        "\n",
        "            confidence = (\n",
        "                float(torch.max(start_scores)) + float(torch.max(end_scores))\n",
        "            ) / 2\n",
        "\n",
        "        answer = self.qa_tokenizer.decode(inputs.input_ids[0][start_idx : end_idx + 1]) # sacamos la respuesta usando esos punteros\n",
        "\n",
        "        # Format and display results\n",
        "        print(f\"\\n{Fore.GREEN}üìù Answer: {answer}{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.BLUE}üéØ Confidence score: {confidence:.2f}{Style.RESET_ALL}\")\n",
        "\n",
        "        print(f\"\\n{Fore.BLUE}üìñ Sources used:{Style.RESET_ALL}\")\n",
        "        for i, (chunk, score) in enumerate(zip(relevant_chunks, relevance_scores), 1):\n",
        "            print(f\"\\nSource {i} (similarity: {score:.2f}):\")\n",
        "            print(textwrap.fill(chunk, width=80))\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"confidence\": confidence,\n",
        "            \"chunks\": relevant_chunks,\n",
        "            \"similarities\": relevance_scores,\n",
        "            \"context_used\": context,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "35je-kw5IPGi"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPTRAG(BaseRAG):\n",
        "    \"\"\"GPT-style generative QA implementation\"\"\"\n",
        "\n",
        "    def __init__(self, pdf_path: str):\n",
        "        super().__init__(pdf_path)\n",
        "\n",
        "        # Load generative model\n",
        "        print(\"üß† Loading FLAN-T5 model...\")\n",
        "        model_name = \"google/flan-t5-small\" # modelo para generar texto, modelo GPT\n",
        "        self.gen_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.gen_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        print(f\"‚úÖ Setup complete! Ready for questions\\n\")\n",
        "\n",
        "    def answer_question(self, question: str):\n",
        "        print(f\"\\n{Fore.GREEN}‚ùì Question: {question}{Style.RESET_ALL}\")\n",
        "\n",
        "        # 1. Get relevant chunks\n",
        "        print(\"\\n1Ô∏è‚É£  Finding relevant context...\")\n",
        "        relevant_chunks, relevance_scores = self._get_relevant_chunks(question, top_k=2) # cogemos todos los fragmentos relevantes\n",
        "\n",
        "        # 2. Prepare prompt\n",
        "        print(\"\\n2Ô∏è‚É£  Preparing prompt...\")\n",
        "        context = \" \".join(relevant_chunks)\n",
        "        prompt = f\"\"\"Answer or complete the question using the information provided in the context. If you're unsure or the answer isn't directly stated in the context, say \"I cannot answer based on the provided context.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # 3. Generate answer\n",
        "        print(\"\\n3Ô∏è‚É£  Generating answer...\")\n",
        "        inputs = self.gen_tokenizer(\n",
        "            prompt, return_tensors=\"pt\", max_length=512, truncation=True\n",
        "        )\n",
        "\n",
        "        outputs = self.gen_model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=150,\n",
        "            min_length=20,\n",
        "            temperature=1.0, # controla como de creativo es el modelo. Bajo, simepre similar. Alta, creativo.\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "        answer = self.gen_tokenizer.decode(outputs[0], skip_special_tokens=True) # decodofocamos la respuesta\n",
        "\n",
        "        # Format and display results\n",
        "        print(f\"\\n{Fore.GREEN}üìù Answer: {answer}{Style.RESET_ALL}\")\n",
        "\n",
        "        print(f\"\\n{Fore.BLUE}üìñ Sources used:{Style.RESET_ALL}\")\n",
        "        for i, (chunk, score) in enumerate(zip(relevant_chunks, relevance_scores), 1):\n",
        "            print(f\"\\nSource {i} (similarity: {score:.2f}):\")\n",
        "            print(textwrap.fill(chunk, width=80))\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"chunks\": relevant_chunks,\n",
        "            \"similarities\": relevance_scores,\n",
        "            \"context_used\": context,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urhUhJbmIPGj",
        "outputId": "04278084-8250-48dd-a6ee-4e445c153dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Unified RAG Demo!\n",
            "Using GPT model for question answering.\n",
            "üîß Initializing RAG system...\n",
            "üìö Loading embedding model...\n",
            "üìÑ Reading PDF: greek_myths.pdf\n",
            "‚úÇÔ∏è  Split into 301 chunks\n",
            "üßÆ Creating embeddings for chunks...\n",
            "üß† Loading FLAN-T5 model...\n",
            "‚úÖ Setup complete! Ready for questions\n",
            "\n",
            "\n",
            "\u001b[32mEnter your question (or 'quit' to exit): \u001b[0mWho is the god of thunder?\n",
            "\n",
            "‚ùì Question: Who is the god of thunder?\n",
            "\n",
            "1Ô∏è‚É£  Finding relevant context...\n",
            "\n",
            "Chunk 1 similarity: 0.45\n",
            "Preview: made up his mind that he would  destroy them all. So he shut up the North Wind\n",
            "in the  caves of √Üolus, and sent forth the South Wind, for the  South W...\n",
            "\n",
            "Chunk 2 similarity: 0.44\n",
            "Preview: called the gods together and  began a terrible battle. !T_h  e Titans tore up\n",
            "enormous  boulders and cast them at the gods, while Jupiter hurled  his ...\n",
            "\n",
            "2Ô∏è‚É£  Preparing prompt...\n",
            "\n",
            "3Ô∏è‚É£  Generating answer...\n",
            "\n",
            "üìù Answer: Jupiter. He has no knowledge of any gods or angels. I think it is clear that Jupiter is god of thunder.\n",
            "\n",
            "üìñ Sources used:\n",
            "\n",
            "Source 1 (similarity: 0.45):\n",
            "made up his mind that he would  destroy them all. So he shut up the North Wind\n",
            "in the  caves of √Üolus, and sent forth the South Wind, for the  South Wind was\n",
            "the wind that would bring the rain.  Clouds gathered over all the earth, and\n",
            "great  drops of\n",
            "\n",
            "Source 2 (similarity: 0.44):\n",
            "called the gods together and  began a terrible battle. !T_h  e Titans tore up\n",
            "enormous  boulders and cast them at the gods, while Jupiter hurled  his\n",
            "thunderbolts and his lightnings in all directio ns.\n",
            "\n",
            "\u001b[32mEnter your question (or 'quit' to exit): \u001b[0mWho is Jupiter?\n",
            "\n",
            "‚ùì Question: Who is Jupiter?\n",
            "\n",
            "1Ô∏è‚É£  Finding relevant context...\n",
            "\n",
            "Chunk 1 similarity: 0.58\n",
            "Preview: h at  Jupiter asked her where she had been, and she described  the beautiful\n",
            "shepherd she had found on her mountain,  and confessed that she had been ...\n",
            "\n",
            "Chunk 2 similarity: 0.58\n",
            "Preview: aling the Ô¨Å re for  man, Prometheus, knowing that Jupiter would be angry,\n",
            "decided to go away for a time on a distant journey; but  ...\n",
            "\n",
            "2Ô∏è‚É£  Preparing prompt...\n",
            "\n",
            "3Ô∏è‚É£  Generating answer...\n",
            "\n",
            "üìù Answer: Shepherd, and shepherd of endymion. He found great value on the mountain, so Jupiter might have saved some of its sheep.\n",
            "\n",
            "üìñ Sources used:\n",
            "\n",
            "Source 1 (similarity: 0.58):\n",
            "h at  Jupiter asked her where she had been, and she described  the beautiful\n",
            "shepherd she had found on her mountain,  and confessed that she had been\n",
            "guarding his sheep.  !T_h  en she begged of Jupiter that since Endymion  was so\n",
            "very, very beautiful\n",
            "\n",
            "Source 2 (similarity: 0.58):\n",
            "aling the Ô¨Å re for  man, Prometheus, knowing that Jupiter would be angry,\n",
            "decided to go away for a time on a distant journey; but\n",
            "\n",
            "\u001b[32mEnter your question (or 'quit' to exit): \u001b[0mIs Jupiter a god?\n",
            "\n",
            "‚ùì Question: Is Jupiter a god?\n",
            "\n",
            "1Ô∏è‚É£  Finding relevant context...\n",
            "\n",
            "Chunk 1 similarity: 0.59\n",
            "Preview: g said this, none of the gods dared  to say a word in defence of mankind. But\n",
            "Prometheus,  the Titan, who was earth-born himself, and loved these  men...\n",
            "\n",
            "Chunk 2 similarity: 0.56\n",
            "Preview: h at  Jupiter asked her where she had been, and she described  the beautiful\n",
            "shepherd she had found on her mountain,  and confessed that she had been ...\n",
            "\n",
            "2Ô∏è‚É£  Preparing prompt...\n",
            "\n",
            "3Ô∏è‚É£  Generating answer...\n",
            "\n",
            "üìù Answer: No Answer is not available. The answer is no. No, in the bible. All the religion does not refer to Jupiter, that is his religion.\n",
            "\n",
            "üìñ Sources used:\n",
            "\n",
            "Source 1 (similarity: 0.59):\n",
            "g said this, none of the gods dared  to say a word in defence of mankind. But\n",
            "Prometheus,  the Titan, who was earth-born himself, and loved these  men of the\n",
            "earth, begged Jupiter so earnestly to sp are  them, that Jupiter consented to do\n",
            "so.  At thi\n",
            "\n",
            "Source 2 (similarity: 0.56):\n",
            "h at  Jupiter asked her where she had been, and she described  the beautiful\n",
            "shepherd she had found on her mountain,  and confessed that she had been\n",
            "guarding his sheep.  !T_h  en she begged of Jupiter that since Endymion  was so\n",
            "very, very beautiful\n"
          ]
        }
      ],
      "source": [
        "def demo(pdf_path: str, model_type: str = \"bert\"):\n",
        "    \"\"\"\n",
        "    Run an interactive RAG demo with specified model type.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        model_type: Either \"bert\" or \"gpt\"\n",
        "    \"\"\"\n",
        "    print(f\"{Fore.CYAN}Welcome to the Unified RAG Demo!{Style.RESET_ALL}\")\n",
        "    print(f\"Using {model_type.upper()} model for question answering.\")\n",
        "\n",
        "    # Initialize appropriate RAG system\n",
        "    if model_type.lower() == \"bert\":\n",
        "        rag = BertRAG(pdf_path)\n",
        "    elif model_type.lower() == \"gpt\":\n",
        "        rag = GPTRAG(pdf_path)\n",
        "    else:\n",
        "        raise ValueError(\"model_type must be either 'bert' or 'gpt'\")\n",
        "\n",
        "    # Interactive loop\n",
        "    while True:\n",
        "        question = input(\n",
        "            f\"\\n{Fore.GREEN}Enter your question (or 'quit' to exit): {Style.RESET_ALL}\"\n",
        "        )\n",
        "        if question.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        rag.answer_question(question)\n",
        "\n",
        "\n",
        "demo(\"greek_myths.pdf\", model_type=\"gpt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}